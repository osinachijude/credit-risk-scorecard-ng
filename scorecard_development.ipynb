# scorecard_development.ipynb
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, classification_report
from sklearn.utils import resample
import xgboost as xgb
import shap
from scipy import stats

# -----------------------------
# 1. Load Data
# -----------------------------
df = pd.read_csv('nigerian_loan_data.csv')
print(f"Loaded {len(df)} records")

# -----------------------------
# 2. EDA
# -----------------------------
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='bureau_score', hue='default_target', bins=50, kde=True)
plt.title('Bureau Score Distribution by Default Status')
plt.xlabel('Bureau Score (0â€“1000)')
plt.ylabel('Frequency')
plt.savefig('bureau_score_dist.png', dpi=150, bbox_inches='tight')
plt.show()

# Default rate
print(f"Overall default rate: {df['default_target'].mean():.1%}")

# -----------------------------
# 3. Feature Engineering
# -----------------------------
# WOE Binning Helper
def woe_iv(df, feature, target):
    df = df[[feature, target]].dropna()
    df_binned = pd.qcut(df[feature], 10, duplicates='drop') if df[feature].nunique() > 10 else pd.cut(df[feature], 10)
    grouped = df.groupby(df_binned)[target].agg(['sum', 'count'])
    grouped['non_events'] = grouped['count'] - grouped['sum']
    total_events = grouped['sum'].sum()
    total_non_events = grouped['non_events'].sum()
    grouped['event_rate'] = grouped['sum'] / total_events
    grouped['non_event_rate'] = grouped['non_events'] / total_non_events
    grouped['woe'] = np.log(grouped['non_event_rate'] / grouped['event_rate'])
    grouped['iv'] = (grouped['non_event_rate'] - grouped['event_rate']) * grouped['woe']
    return grouped['woe'].to_dict(), grouped['iv'].sum()

# Apply WOE
woe_maps = {}
iv_values = {}

for col in ['bureau_score', 'debt_to_income_ratio', 'monthly_income_ngn', 'loan_amount_ngn']:
    woe_map, iv = woe_iv(df, col, 'default_target')
    woe_maps[col] = woe_map
    iv_values[col] = iv

# Encode
for col, woe_map in woe_maps.items():
    df[f'{col}_woe'] = pd.cut(df[col], bins=woe_map.keys(), labels=False).map(woe_map)

# Categorical dummies
categoricals = ['employment_status', 'residence_type', 'region', 'loan_purpose', 'application_channel']
df_encoded = pd.get_dummies(df, columns=categoricals, drop_first=True)

# -----------------------------
# 4. Model Training
# -----------------------------
features = [c for c in df_encoded.columns if c.startswith(('woe', 'age', 'tenure', 'interest_rate')) or c in [
    'gender_M', 'gender_Other', 'existing_loans_count'
]]
X = df_encoded[features]
y = df_encoded['default_target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict_proba(X_test)[:, 1]

# XGBoost
xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict_proba(X_test)[:, 1]

# Performance
print("Logistic AUC:", roc_auc_score(y_test, y_pred_lr))
print("XGBoost AUC:", roc_auc_score(y_test, y_pred_xgb))

# -----------------------------
# 5. Scorecard Creation (Logistic)
# -----------------------------
feature_names = X.columns
coefficients = lr.coef_[0]
intercept = lr.intercept_[0]

# Standard scorecard scaling: Base score = 600, PDO = 20
factor = 20 / np.log(2)
offset = 600 - factor * np.log(15)  # Odds at 600: 15:1

# Score per feature
scorecard = []
for feat, coef in zip(feature_names, coefficients):
    points = round(-coef * factor, 0)
    scorecard.append({'Feature': feat, 'Points': points})

scorecard_df = pd.DataFrame(scorecard)
scorecard_df['Total_Intercept'] = round(intercept * factor + offset, 0)

# Save scorecard
scorecard_df.to_csv('application_scorecard_points.csv', index=False)

# -----------------------------
# 6. Validation Metrics
# -----------------------------
from scipy.stats import chi2_contingency

def calculate_psi(expected, actual, bins=10):
    expected_bins = np.histogram(expected, bins=bins)[0] / len(expected)
    actual_bins = np.histogram(actual, bins=bins)[0] / len(actual)
    psi = np.sum((expected_bins - actual_bins) * np.log(expected_bins / actual_bins + 1e-12))
    return psi

psi_bureau = calculate_psi(y_train.map(lambda i: np.random.choice(df['bureau_score'])), 
                           y_test.map(lambda i: np.random.choice(df['bureau_score'])))
print(f"PSI (Bureau): {psi_bureau:.3f} (Stable)")

# -----------------------------
# 7. SHAP for XGBoost
# -----------------------------
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, show=False)
plt.savefig('shap_summary.png', dpi=150, bbox_inches='tight')
plt.show()